==XXX Design experiments==
In ISEM paper, we compare: Percentage of Nos with Real GDP growth rate.

Since we cannot compute Percentage of Nos, we compare:

SUM of Nos from GESIS with Employment rate, by sex from Estatwrap from Germany.

This would show: Integration of Germany. 

In ISEM paper, we had with respect to size of datasets:

1) 2 DS with 20,268 triples
3a) 4 DS with 24,636 triples
3b) 8 DS with 35,482 triples

We now would like to extend this.

==2014-05-02==
* No success with entity consolidation for Drill-Across.
* Problem: If I just replace all occurrences of a URI with the canonical form, I have the problem 
that Estatwrap is using the same URI for Dim / Hier / Level, whereas GESIS uses different ones for 
Dim / Hier / Level. This is because Estatwrap uses rdfs:range nuts:Region whereas GESIS uses a code 
list #list.
* What do we want? 
** Canonical identifier for dimension? gesis:geo (selected because first dataset)
** Canonical identifier for hierarchy? gesis:geo (since estatwrap hierarchy is the same as estatwrap dimension) - However, 
** 
* How about defining that #list is also owl:sameAs estatwrap:geo? In this case, 

Todo: 
A first step would be to "consolidate" dimensions and members, i.e., to only return canonical identifiers.

A second step would be to only use those canonical identifiers (let's make it the first occurence) in MDX queries (so that those identifiers can be found).

A third step would be to change the generation of OLAP operators to use the right Linked Data identifiers for the respective dataset. However, how to know the correct identifier? 

A forth step would be to change the Drill-Across operator to consider equivalences.

===Convert-Cube===
* Now I use inputcube1, inputcube2 and outputcube.
* Also, I have written all reconciliation correspondences.
* Next steps:
* Implement Merge-Cubes
* Compare GDP per capita computed and GDP per capita directly.
* Extend the case study with external datasets

===Entity consolidation===
SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsAGGFUNCSUM]} ON COLUMNS, 

{Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS 

FROM [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] 

WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FdicXXX2FgeoXXX23DE]},{[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]})


Logical query plan: Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00 AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00 AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

What problems do we see here?

Assume we really set estatwrap:geo owl:sameAs gesis:geo. This is what we do here: http://people.aifb.kit.edu/bka/Public/cube_additionalRDF.rdf

Then, both dimensions may have the same two codelists or define different ranges, e.g., nuts:region.

In ISEM paper, we 1) use canonical identifiers for dimensions and members when storing the metadata and 2) from the multiCubeDimensionsClosureTable mapping canonical values to dimension URIs use all dimension URIs in a OR filter clause for querying the data (which of course makes the query for data quite slow). 

However, something similar, we could do also: 

1) When querying for dimensions / hierarchies / levels and members I consolidate and use a canonical identifier which I map to original dimension URIs. Thus, we will have as identifiers new canonical ones. XXX: Could we not instead allow all possible identifiers?

2) When querying for facts I use all dimension URIs in an OR filter clause and all member URIs in another OR filter clause.

Why does our current query not work? Since we filter for gesis:geo although for estatwrap we would need to filter for estatwrap:geo. 

There are different possibilities:

* We could leave the OLAP-to-SPARQL algorithm as it is and make sure that the proper dimensions are used as input to it. However, we would need to extend Drill-Across to consider equal dimensions and members.
* We could extend the OLAP-to-SPARQL algorithm to query for all possible dimensions or members.

A first step would be to "consolidate" dimensions and members, i.e., to only return canonical identifiers.

A second step would be to only use those canonical identifiers (let's make it the first occurence) in MDX queries (so that those identifiers can be found).

A third step would be to change the generation of OLAP operators to use the right Linked Data identifiers for the respective dataset. However, how to know the correct identifier? 

A forth step would be to change the Drill-Across operator to consider equivalences.

How have I done it for ISEM paper? I created canonical identifiers for metadata and queried simply for every possible URI in data.

What is different with triple stores? I have a query on the global cube with canonical identifiers. When I create the initial query plan, I use elements from the single datasets. This works in case the elements are directly represented in the single datasets. However, in case of canonical identifiers, I do not directly have the elements represented with this name. One thing I could do is to use those canonical identifiers still, but internally map.

Problem: If I just replace all occurrences of a URI with the canonical form, I have the problem that Estatwrap is using the same URI for Dim / Hier / Level, whereas GESIS uses different ones for Dim / Hier / Level. This is because Estatrap uses rdfs:range nuts:Region whereas GESIS uses a code list #list.

How was that in ISEM paper? In ISEM paper, I did not represent Hierarchies, yet. I assumed every dimension to have exactly one hierarchy and level with the same name as the dimension (see findDimensionHierarchies). 

==2014-05-01==

===First, integrate properly GESIS and Estatwrap===
Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

... Now, we see that we need to start integrate the datasets. How to do that, entity consolidation 
as in ISEM paper?

What exactly do we need to do? 

===Dice for integration===
The problem seems that we dice only for the gesis:geo dimension.

First, check whether dice works at all:

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM]} 
ON COLUMNS, 

{Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS 

FROM [httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds]

WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300]},{[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]})

Note, we are not allowed to ask (e.g., dice) for non-existing members.

Question: How is dice defined?

According to OLAP4LD demo paper, Dice removes for every possible member combination on axes filtered dimension members. This probably is not completely true. We dice as specified in WHERE clause (filter axis). A filter axis would return member combinations (positions) from a fixed set of levels (or hierarchies) from the filter axis. Thus, for filter axis (just as for the column and row axis), we create a list of member combinations (positions). In filter axis, either we have a list of members, or we have a list of lists. We want to have returned a list of positions. Every position is a possible combination of members of a fixed set of levels. 

In case there are members, directly, we return 

Execute logical query plan: Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_0)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {})

This seemingly "error" to use geo for both OR sides is ok since every OR should have the same "signature".

Watch out: Saiku would use CrossJoin for representing AND: CrossJoin({[httpXXX3AXXX2FXXX2FpublicYYYbZZZkaempgenYYYdeXXX3A8080XXX2FcikXXX2F1141391XXX23id]}, {[httpXXX3AXXX2FXXX2FpublicYYYbZZZkaempgenYYYdeXXX3A8080XXX2FvocabXXX2FusZZZgaapZZZ2009ZZZ01ZZZ31XXX23Assets]})

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsAGGFUNCSUM]} ON COLUMNS, {Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS FROM [httpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FdicXXX2FgeoXXX23DE]}, {[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]}}

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsAGGFUNCSUM]} ON COLUMNS, {Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS FROM [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] WHERE {[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FdicXXX2FgeoXXX23DE]}

Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

... Now, we see that we need to start integrate the datasets. How to do that, entity consolidation 
as in ISEM paper?

===Use dataset name in measure name (as in ISEM paper)===
* Had to replace dataset name with "" in property and variable.
* Currently only for SUM.

===Problem: Drill-Across over cubes with same measures===
* Problem is that in the MDX query, there is no way to distinguish between the measures.
* Possible solutions
** Decide from the ordering: The first obsValue, the second obsValue. However, this would need to be generic.
** Merge measures: In resulting cube, only have one measure. If there are several facts that have the same dimension member combination, we would throw an error. This would allow to union cubes. 
** Further automatically distinguish measures by the cube. Thus, not only sdmx-measure:obsValueAGGFUNCAVG and sdmx-measure:obsValueAGGFUNCCOUNT and sdmx-measure:obsValue, but sdmx-measure:obsValueCube1AGGFUNCAVG ...

Question:

* How many measures does the "global cube" have? Drill-Across should be defined as combining measures from several cubes (otherwise, would make it more complicated as is). Processing-wise, it would not make a difference. The global cube should have measures for each single measure in the cubes. This is how we defined it in ISEM paper: Note, in order to compare metrics from separate cubes - different from Dimensions and Members - Measures always need to denote different metrics, even though they may be described by the same property or linked by owl:sameAs. Thus, how about: 

Problem:

ALLBUS/ZA4570v590.rdf defines an rdf:range of xsd:decimal, Estatwrap defines an rdf:range of xsd:double. As a result, the metadata queries for measures return two different measures with the same name. 

Solution:

The real problem is that publishers use sdmx-measure:obsValue differently. They set a range, although the official publishers have not done that. In my opinion, in this case, publishers should create sub-properties of sdmx-measure and then "extend" it.

As a workaround, for now, we simply always assume xsd:decimal as data-type. However, this is a quick-fix.

Question: What dataset does the measure httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG use? It results in this:

+--+------+-------------+
|  | 2005 |             |
|  | 2006 |       93.86 |
|  | 2007 |             |
|  | 2008 |       95.28 |
|  | 2009 |             |
|  | 2010 |             |
|  | 2011 |             |
|  | 2012 |             |
|  | 1980 |             |
|  | 1991 |             |
|  | 1992 |             |
|  | 1994 |             |
|  | 1996 |             |
|  | 1998 |             |
|  | 2000 |             |
|  | 2004 |             |

The new measure httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsAGGFUNCSUM does not result in anything.

In summary, the problem is that: Either we do select a general measure. Then, both OLAP-2-SPARQL algorithms return results. Nested loop would find the overlaps and probably return the first datasets measure since the global cube does ...? Good question.

The problem is the following: The "global" cube describes all measures, dimensions etc. But Drill-Across assumes the metadata of the first dataset. This is wrong since we need the union of measures etc. 

Solution: The Metadata about the "global" cube needs to be a super-set of the Metadata of the resulting Drill-Across operators. Especially, since Drill-Across can be nested, also. Thus, we need to distinguish:

* createMetadata()
* createData()

One thing I do not like currently: 

1) Partly, we create the metadata using SPARQL queries in metadata queries of Linked Data Cubes Engine. 
2) Partly, we create the metadata directly on given metadata in analytical operators.

The thing is that the analytical operators create many more different data cubes, e.g., slices of the overall data cubes.

Therefore, we probably cannot find synergies, here. 

Plan: 

To have the operators create the metadata, also.

Allow Base-Cube, to also return metadata of the "global cube".

Unfortunately, metadata queries with their "restrictions" are still different from simply loading the cube metadata. Therefore, this refactoring is postponed.

Instead, I go ahead and add the metadata processing to Drill-Across iterator.

Quick-fix for now: The measures are created specifically, all others are not.

If I ask for the same measure twice, the result would always be the first measure (since we cannot distinguish
between different datasets)

Therefore, next goal is to -- similar to ISEM paper -- to also use the dataset name in the measure name.

==2014-04-30==
* Works now. Problems were:
* Not clear what infos are extracted from original MDX parse tree.
* Now, measure list is created from MDX parse tree.
* Measure list is list of members, so, no header.

==2014-04-24==
* Still problem of Drill-Across
* Status (Query + Result)

Drill_across_QueryTest

<pre>

Logical plan:Drill-across(Slice (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), 

Slice (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tsdec420_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#sex}

))

+ 

http://olap4ld.googlecode.com/dic/geo#US; 2011; 148.0; 1; 148; 70.433333333333333333333333;

</pre>

Example_QB_Datasets_QueryTest.testDrillAcrossEstatwrapGDPpercapitainPPS_EurostatEmploymentRate

<pre>

Logical query plan: Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), 

{http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCCOUNT}), {}), 

{http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tsdec420_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#employment_rate}), {}), 

{http://ontologycentral.com/2009/01/eurostat/ns#sex}), {}))

+


|  |   US   |  | 2005 |       159.0 |       159.0 |            159.0 |
|  |        |  | 2006 |       155.0 |       155.0 |            155.0 |
|  |        |  | 2007 |       152.0 |       152.0 |            152.0 |
|  |        |  | 2008 |       147.0 |       147.0 |            147.0 |
|  |        |  | 2009 |       147.0 |       147.0 |            147.0 |
|  |        |  | 2010 |       148.0 |       148.0 |            148.0 |
|  |        |  | 2011 |       148.0 |       148.0 |            148.0 |

</pre>
 

==2014-04-02_v2==
* Towards paper: Derived datasets 
* Refactored Visitor and Iterator names.
* Now, we have: 
** Visitor and Iterator for Drill-across and OLAP-to-SPARQL.
** Visitor and Iterator for Derived Datasets, Convert.
* See /media/84F01919F0191352/Projects/2014/paper/paper-macro-modelling/experiments/ for dumps created in between.

==2014-04-02==
* Drill-across operator and iterator work now. However, note, that for iterator neither reasoning is done nor arbitrary ordering of shared dimensions in result is considered.
* Bugfix in Olap2SparqlAlgorithmIterator: Not properly updated measures metadata.
* Working on test case: testDrillAcrossEstatwrapGDPpercapitainPPS_EurostatEmploymentRate
* Question: Do we not need to make DrillAcrossSparqlIterator properly update the metadata? Do we need to create the metadata of a new cube, just like we do for metadata queries? 

<pre>
The problem with Drill-across currently is that the initial query plan adds all projected measures and adds all sliced dimensions.

Also, the initial query plan uses elements from the global dataset, so that we cannot distinguish between
measures from different datasets.

However, need to make sure that only existing measures are projected. 

Solution: Before projection, check whether existing in data cube.

However, there we have the problem that the initial query plan uses Base-cube...

The key problem is in createInitialQueryPlanPerDataSet() where we ask the Linked Data Engine for the metadata of each single dataset, 
but also consider the metadata of "metaData.cube".

</pre>

* Question: When and how is metaData.cube set? It is set when creating the metadata. The metadata queries now also create a global cube.
* Question: Where lies the error?
** Maybe the metadata is not correctly build? How can I check?

* Question: Do I need to make the DrillAcross Iterator return properly modelled metadata?

* Problem: Our OLAP-to-SPARQL algorithm needs all SlicesRollups dimensions to be part of the queried data cube. However, our slicedDimensions is created for all datasets when creating the initial logical query plan. 

* Problem: All multidimensional elements in MDX query first belong to the global cube. Now, before being used in the query to the single data cube, we have to check that they actually refer to an element of the dataset.

* This is the case for projections, where we specifically project for all existing measures.

* This also is the case for slices, where we slice for any dimensions from the global cube not mentioned in query.

* How to deal with this issue?
** OLAP-to-SPARQL algorithm has a well-defined meaning.
** We can either create a correct initial query plan
** Or, we can later adapt the initial query plan
** Or, when we create the inputs to the OLAP-to-SPARQL algorithm, we ignore some parts of the query plan

* I think, if possible, we work on the initial query plan.
* Thus, next time, we try to filter non-existing projected measures and sliced dimensions.


==2014-03-22==
* Metadata queries can return global cube

==2014-03-18==
* For the first time, a GDP per capita calculation could be done in one go.

==2014-03-05==
* Refactored LDCEs:

What LogicalOlapOperatorQueryPlanVisitor do we have?

* Olap2SparqlSesameDerivedDatasetVisitor <= Was extended for Drill-across but now is more going towards "deriving datasets". Maybe, only for ConvertContext, we should materialise a new dataset.

* Olap2SparqlSesameVisitor <= Should also for LDCX eventually support Drill-across but for now does not properly define logical operators. Assumes that rollup is executed for all levels, even if not rolled-up.

To have several engines has the disadvantage that they share some functionality/logic, e.g., metadata queries, loading of cubes.

If I see this correctly, we may for now have one engine that uses different "LogicalOlapOperatorQueryPlanVisitors".

This one and only EmbeddedSesame engine includes possible preloads() in Iterators. Preloads() use LoadCube(). LoadCube() is provided as a method. Also provided as methods are Normalise() and IntegrityConstraints(). They should be run, after one or more datasets have been loaded.

LoadCube() can later be better implemented using Data-Fu. I assume that the current loading is better than the old.

We may also add getCubes() as Operators/Iterators.

We now include both "implicit measures" as well as measures without explicit aggregation function. We have also added these measures as members to getMembers().

==2013-11-05==
* Added example performance evaluation files in "testresources"
* fixed error spelling of integrity check IC-8 of spec and continued working on slicer 
* slicer can now query for certain slices (see first test)

==2013-10-23==
Refined:
* LDCX_Performance_Evaluation_LogParse_Experiments.java - Parsing log files of olap4ld and retrieving
events for interesting metrics.
* LDCX_Performance_Evaluation_XmlaTest.java - Issuing workload for performance evaluation
* LDCX_User_Study_Evaluation_XmlaTest.java - Test Case for User Study queries.

==2013-10-20==
* More fine granular logging that can be parsed using regex as done in LDCX_Performance_Evaluation_XmlaTest_LogParse_Experiments.
* LDCX_Performance_Evaluation_XmlaTest with tests for performance evaluation
* LDCX_Performance_Evaluation_XmlaTest_LogParse_Experiments with experiment using exrunner by
Günter Ladwig for inserting log parser results into a sqlite dabase for querying and visualising
using gnuplot.
* Refined one of the example datasets (Eurostat).
* Added example for exrunner by Günter Ladwig: SortExperiments.java
* Exrunner requires (not uploaded)
** exrunner-0.0.1-SNAPSHOT.jar
** guava-15.0.jar
** sqlite-jdbc-3.7.15-SNAPSHOT-2.jar

==2013-10-3==
* Pooling problem workaround implemented: http://linked-data-cubes.org/index.php/Olap4j-xmlaserver#Problem:_Pooling_of_data_sources

==2013-10-1==
* Pooling problem investigated, see: http://linked-data-cubes.org/index.php/Olap4j-xmlaserver#Problem:_Pooling_of_data_sources

==2013-09-29==
* We now throw more OlapExceptions in Linked Data Engines (Sesame), since we hope that this
makes the system more robust in case of errors.
* A prepared olap statement is used by XmlaHandler.java 
(statement = connection.prepareOlapStatement(mdx);). Afterwards the cellset is 
created (cellSet = statement.executeQuery();). This means in our implementation, that the cellset
is populated twice and the linked data engine is queried twice (and logging is done, twice). Thus,
we have adapted cellset to only populate the cells when actually cells are asked for; otherwise, only
the metadata is created and can be queried from the prepared olap statement if necessary.
* Unfortunately, the prepared statement does not help us in any way to save resources. Therefore, we
have disabled it in Olap4ldPreparedStatement.java. 

==2013-09-28==
* More detailed logging, per connection. Unfortunately, no session tracking, yet.
* Better labels for SSB dataset

==2013-09-27==
* TODO: 
** Better understanding for users. Unfortunately, currently there is a JavaScript error in LDCX.
** Even more robustness? 
*** Currently, if I cannot store any more in the triple store, I just throw an error. After reload, a new triple store should be filled?
*** If a query is too big, java script LDCX currently does not complain, can be improved?
*** According to Günter und Daniel, asking for the amount of free memory is not a best practice. 
Instead 1) I should properly test the system and see whether there are errors 2) I should put user
restrictions on the system, e.g., you cannot analyze datasets with such and such amount of triples.
**** Added triple count check and magic number 100,000 triples max.

** LDCX pivot visualisation often wrong (first column, if we use several dimensions on rows?)

==2013-09-26==
* Why does Edgar example dataset does not fulfill constraint 12? [http://www.w3.org/2011/gld/validator/qb/qb-test?upload=upload-2013-09-25T20-51-51-104] 
** This is in more detail described here: http://linked-data-cubes.org/index.php/Displaying_of_background_information_useful_for_interpretation
** Edgar would need to be improved

* Robustness
** The system is still not very robust, since queries may ask for too many columns and rows to be visualized (in JavaScript on client? Or to populate result set on server?). 
*** One solution might be JSON as described here: http://www.blogger.com/comment.g?blogID=5672165237896126100&postID=9002652898272515547
*** Another solution would be to properly implement NON EMPTY COLUMNS and NON EMPTY ROWS and to set this up as default in the system. This way, no impossible queries for a non too big file should be possible.
*** Question: Does olap4ld cope with a big query?

* For robustness: Properly implement NON EMPTY CLAUSE:
** Make test case: We extend ssb example. <= result: NON EMPTY works just fine, the problem is rather the huge XMLA result for large queries. Any cell that is visible is mentioned in the xmla result.
** More info see http://linked-data-cubes.org/index.php/NON_EMPTY_Clause_in_MDX
** As a result, we should be using NON EMPTY for both rows and columns.

==25 Sep 2013==
* DONE
** For XMLA Cube Discovery (and later, Dimension-Discovery...), I want to load in the respective 
metadata (and, for now, data). For a query, however, I do not necessarily want to load everything
again.
*** Caching is done in two cases: 1) By filling the triple store. 2) By populating 
the metadata object lists.
**** 1) Currently, a triple store is created when the EmbeddedSesameEngine is created which is
done when the Driver is loaded which should be only once per XMLA-Connection. The triple store 
is set back (rollback), when Connection.setCatalog() or Connection.setSchema() are called which is done
in xmlaserver every time in RowsetDefinition a XmlaResponse is populated (populateImpl()). 
For instance: populateImpl -> setCatalog() -> getCatalog() -> populateCatalog() -> 
catalog.getSchemas() -> schema.getCubes() -> populateCube() -> cube.getDimensions() -> 
populateDimension() -> dimension.getDescription() -> dimension.getHierarchies() -> 
firstHierarchy.getLevels()... <= TODO: If we assume that Discover.Cubes is called at the very beginning, 
the triple store should not be rollback for every discover method. It actually should never be
generally rolled back but dynamically "updated" (there is no open rollback functionality): 
1) Disable rollback. 2) Populate repository as much as possible and if we cannot populate anymore, 
give error. For now, lets see when a new repository is created.  
**** 2) The metadata object lists currently are deferred lists which get populated 1) if they are 
new. 2) if explicitly a cube is asked for.
** Test Cases do not work any more with constraints (should I disable them, again?)
*** Instead of adding new normalization algorithms (first done, but commented for skos), I changed
the example of ssb001.
** Problem: Some integrity constraints are too complex to be run on larger datasets, esp. those
which need to touch every observation (e.g., IC-12). 
*** Possible solutions: 1) For now, we cannot run these integrity constraints and comment them. 2) We only run
them if the size of the dataset is small and "small" is depending on the size of memory. 
For instance, if free memory size is twice as big as current load of data. 3) We
only run them in a specific cube dataset debugging mode.
*** Current solution: Had to use 3) since 2) did not work.
*** Planned solution: Maybe own better implementation of integrity checks or use more memory 

==22 Sep 2013==
* Why does Eurostat example agg dimension is not shown? There, we should have a test case. 
Solution: Had to define range and datatype for measure. Done for example.
* For eurostat example, I also had to adapt the dimension names (which were ontologycentral).
** Also, had to adapt so that codelist and observation coincidate. Why did we not realize that observation values did not align with code list values?
* Since the error before has not been detected, I added more integrity constraints from spec
** watch out, also inScheme had to be done to fullfil integrity constraint

==21 Sep 2013==
* Problem: Apparently, loading "http://estatwrap.ontologycentral.com/id/../data/tec00114" fails, 
whereas loading "http://estatwrap.ontologycentral.com/data/tec00114" does not.
* Improved logging, see http://www.linked-data-cubes.org/index.php/Logging_Olap4ld
* new estatwrap example: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds
** had to adapt it: every observation links to #ds. Structure has certain uri (with hash #).
* added SMARTDBWRAP example
* refined test suite Example_QB_Datasets_QueryTest

==20 Sep 2013==
* LDCX should be working on example datasets
** Estatwrap is mostly offline. Therefore, we created example datasets.
** Edgar, unfortunately, most filings cannot be represented, currently.
** Show proper dataset name for Edgar

* Improve LDCX
** Monitoring usage (Session + Logical Query + Successful/Unsuccessful error) = normal running logger.
** Logging levels: 
    FINEST <= not used
    FINER <= not used
    FINE <= not used
    CONFIG <= debugging
    INFO <= usage monitoring (only the most important bits and pieces)
    WARNING <= possible errors
    SEVERE <= not used


* More robust w.r.t. memory usage
** Heap space bigger?
** Build in security that too big files are not loaded and that we store not too many triples in the repository (maybe should be possible to setup) 

* Demo should work on hierarchies, also
** Extend ssb example with skos:ConceptScheme possibilities

==19 Sep 2013==
* Have been working on Edgar dataset example
* Problem: Suddenly, LDCX did not work, anymore. Solution: There was an error in the JavaScript. Use Firebug and look at error console.
* Repository is now rollback() if catalog or schema are set, not get.
* Added new version of Example Edgar dataset
** Allow any Edgar dataset from feed - *not necessary for demo*

==16 Sep 2013==
* Problem: If we create measures based on a measure without aggregation function, and we select several measures,
we do not create separate graph patterns. *solved*
* olap4ld tests were running endless since parentuniquemember with [null] had to be searched for.
Bug of having [null] names for "null"/null (according to spec) removed in Connection.Handler and Olap4ldUtil. 
* Status: Yahoo! Finance tests are not successful. Yes, they are, there apparently was some temporal error.

==12 Sep 2013==
* When transforming an MDX query to a query in MdxMethodVisitor, we distinguish whether a cube
is identified or not, via this.cube that is stored when visiting the from clause.

==8 Sep 2013==
* Problem: 
** How to ask for representation of Physical Query Tree?
*** simply allowing to ask LDE for last Physical Operator Tree
*** OR: How about defining better logging mechanism? 
*** Currently, we define logging level at Driver. We now try to log to a file.
* More logging in Slicer

==6 Sep 2013==
* Slicer_QueryTest.java [1] now executes all 1-dimensional OLAP queries to example cube.

==5 Sep 2013==
* Restrictions now use URI representation, but also are able to convert from MDX representation.
* Slicer_QueryTest.java [1] now contains test "test_example_ssb001_slicer_oneDim()", that creates 
all 1-dimensional OLAP queries to example cube

==2 Sep 2013==
* Renamed LD_Cubes test case to "Example_QB_Datasets_QueryTest".
* Logical Olap Op DiceOp now receives List<Node[]> of cube that is
used in query processing.
* removed all errors in sources
* Deactivated not successful tests in Example_QB_Datasets_QueryTest.
* Logical Olap Op SliceOp and RollupOp now receive List<Node[]> of cube that is
used in query processing. 
* OlapOps are now completely using Node[] with URI representation (not MDX representation) as inputs

==31 Aug 2013==
* Logical Olap Op ProjectionOp now receives List<Node[]> of cube that is
used in query processing.

==30 Aug 2013==
* Logical Olap Op BaseCube now receives List<Node[]> of cube that is
used in query processing.
* Status of junit test LD_Cubes_Explorer_QueryTest.java: Only tests
related to the setDatabase problem and Edgar loading do not succeed.

==2013-08-16==
* Added methods to metadata objects to get List<Node[]> representation to be used in olap 
operators (as an interface to Linked Data Engine).

==2013-08-15==
* Logical OLAP Operators (http://www.linked-data-cubes.org/index.php/Olap4ld_Query_Optimizer)
* Changed Visitor to LogicalOlapOperatorPlanVisitor

==2013-08-14==
* If now aggregation function is found, we automatically create new measures with certain
aggregation functions (simply added aggregation fnct name to the end). 
However, WATCH OUT, BIND-syntax of SPARQL is complicated: You should not be
using the variable that you are binding to in other patterns (see, e.g., ?MEASURE_AGGREGATOR_PART)

==2013-08-07==
* Improved exception handling. Now, LinkedDataEngine also returns OlapExceptions which are forwarded
to the using engine in order to make transparent errors to users.

==2013-07-12==
* Removed sesame libraries. Sesame is to be added externally when using sesame engines in olap4ld

==2013-07-11==
* Full spec tests on test data
* Full spec test better loging for users, independent from logging level. Better log-out.

==2013-07-10==
* Eurostat examples. Have English labels.
* Extended test cases
* Retrieving of data so far very simple: 
** rollback every time we ask the database catalog for its schema.
** loading of uris if we check whether something is a cube; loading ds, then dsd.
** loading of other uris just by coincidence if we check if they are cubes.

==2013-07-07==
* Nested crossjoins are possible, now.
* Examples listed in demo
* ld-cubes-explorer added
* Problem: Have caption shown instead of elements.

==2013-07-05==
* Problem: GetCubes() auf dem Schema gibt uns keinerlei Cubes, da das System nicht weiß, woher
er sie nehmen soll. Daher setzen wir auf getMeasures(Cube), getDimensions(Cube) etc., müssen hier
jedoch sicherstellen, dass zumindest der Datensatz aufgelöst worden ist.
** Lösung: We check the restrictions and at least check whether dataset is resolved (in which case,
the datastructuredefinition should have been resolved as well).
* with setCatalog() in Connection, I reset the LinkedDataEngine from now on. <= does not work!
** We reset at getSchemas() of Catalog. Also, we now load only if instance qb:DataSet.
* We inserted checks from specification for when qb:DataSet is loaded.
* Problem: Beim Identifizieren von Identifiern, muss ich durch alle elemente gehen, dabei frage ich 
auch, ob es sich um einen Cube/DS handelt. Lösung: Bevor ich es mache, überprüfe ich erst, ob es sich 
tatsächlich um ein DS handelt.

==2013-07-4==
* Problem came up that the following elements have the same name in our current model:
** Dimension, Dimension without codelist Hierarchy, Dimension without codelist Level
** Hierarchy, Regular hierarchy level
** Since we do not need to refer to those sub-elements, directly, but rather will be asking for all 
members of the dimension, we do not need to consider this.
* Measure DataType is now interpreted from rdfs:range.
* Watch out for the right name in MDX query.
* Sesame SPARQL adaption:
** Picky with empty spaces, added serveral
** (COUNT(?variable) as ?variable)
* xmla-server + xmla4js was quite picky about some non-implemented methods in olap4ld taken over 
from olap4j/mondrian (e.g., Olap4ldDatabase.getUserName()). Fixed those with workarounds. 
* We name Catalog and Schema the same due to compatibility reasons with xmla-server/xmla4js
* We have added measureList to CellSet as added metadata.

==2013-07-03==
* xml2nx adapted to Sesame SPARQL/XML output with linebreaks
* added manually created representative SSB example
* added Sesame SPARQL templates

==2013-07-02==
* Read in first location.
* Problem: A data cube is an instance of qb:DataSet. What do we
typically store with the qb:DataSet? For sure the outgoing link to dsd,
the outgoing link to all of its slices that in turn link to the
observations. Sometimes, we may have the observations already in the ds
location.
* Ideally, we get the following from...
** DS: Links from observations / Links to slices that in turn link to observations, rdfs:label, 
rdfs:comment, qb:structure.
** DSD: All information about the DSD, including code lists.

* For now, we assume (and adapt the SSB dataset accordingly):
** DS: Links from observations, rdfs:label, qb:structure
** DSD: All measure properties, dimension properties, code lists...

* Note in spec: If a dimension property has a qb:codeList, then the value of the dimension property on every qb:Observation must be in the code list. 
* Note in spec: Every qb:Observation has exactly one associated qb:DataSet. 
* Note in spec: Every qb:DataSet has exactly one associated qb:DataStructureDefinition. 
* Note in spec: "Every dimension declared in a qb:DataStructureDefinition must have a declared rdfs:range."
* Note in spec: "Every dimension with range skos:Concept must have a qb:codeList." <= This means, 
we do not necessarily need a code list in many cases. But, if we have a code list, then:
* "If a dimension property has a qb:codeList, then the value of the dimension property on every 
qb:Observation must be in the code list."

==Status 2013-07-04==
* Decided for own example based on example in spec:
** sdmx dimension: http://publishing-statistical-data.googlecode.com/svn/trunk/specs/src/main/vocab/sdmx-dimension.ttl
** code list sex: http://publishing-statistical-data.googlecode.com/svn/trunk/specs/src/main/vocab/sdmx-code.ttl#sex
** folder of own online linked data files: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/

==Status 2013-07-03==
* Note in spec: Every qb:DataSet has exactly one associated qb:DataStructureDefinition. 
* Note in spec: "Every dimension declared in a qb:DataStructureDefinition must have a declared rdfs:range."
* Note in spec: "Every dimension with range skos:Concept must have a qb:codeList." <= This means, 
we do not necessarily need a code list in many cases. But, if we have a code list, then:
* "If a dimension property has a qb:codeList, then the value of the dimension property on every 
qb:Observation must be in the code list."
* Note in spec: Every qb:Observation has exactly one associated qb:DataSet.

==Status 2013-06-26==
* Added SSB_001 test data that I simply shortended. Can be looked up via: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/custlevels.ttl#dsd
* Added [http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/custlevels.ttl#dsd] as prefixes (We need a better solution, here)
* Worked now further on http://intra.b-kaempgen.de/dawiki/index.php/Creating_an_OLAP4LD_test_environment_and_using_Sesame 
or now at http://www.linked-data-cubes.org/index.php/EmbeddedSesameEngine 
* Changed the fact that if specific element of DefferredList is queried, we do not "populate".


==2013-06-26 (before)==

* Warum haben die Dimensionen alle zwei Englische rdfs:label? 

===Problem: Saiku calls <Measures>===
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?HIERARCHY_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_CAPTION ?DIMENSION_UNIQUE_NAME as ?LEVEL_NAME "N/A" as ?DESCRIPTION "1" as ?LEVEL_NUMBER "0" as ?LEVEL_CARDINALITY "0x0000" as ?LEVEL_TYPE  from <http://fios:saiku>  where { ?CUBE_NAME qb:component ?compSpec . ?compSpec qb:dimension ?DIMENSION_UNIQUE_NAME. FILTER NOT EXISTS { ?DIMENSION_UNIQUE_NAME qb:codeList ?HIERARCHY_UNIQUE_NAME. }   FILTER (?CUBE_NAME = <http://public.b-kaempgen.de:8080/archive/MA/2007-06-06#dsd>)  FILTER (?DIMENSION_UNIQUE_NAME = <Measures>) } order by ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?LEVEL_NUMBER 


===Open issues===

* Reasoning does not work
** If we enable reasoning, the queries take much too long
** Also, we do not know what URIs are given to the members.
** Possible solution: We need to further normalise the data to use one specific uri per resource.

* Queries are very slow. Possible solutions:
** We could define code lists (hierarchies etc.)
** We could enable caching
** We could only gradually populate the multidimensional data

** How about predefining codelists?

* CAPTIONS
** We only use english captions
** We use skos:notation for members
** We use rdfs:label for everything else

* How about querying over several cubes together?
** We have to define the measures clearly. For SEC / Yahoo what is the appropriate aggregation function?
** Solution: AVG seems a good candidate for now. 
** Another solution: We separate the measures.

* How about the Member function?

* How about the time information?
** One part of the problem: In the current implementation, we cater for the special situation where
the observations use Literal values but we still would like to define code lists. In those cases,
we so far allowed to use skos:Concept and to give them skos:notations that could be used in the
observations. In FIOS 2.0, this would be the case for dates and for segment. However, we already use
skos:notation as a (unique) caption for a member. Therefore, we change this assumption and take
Literal observations automatically as "degenerated dimensions".
** Also, I am not sure whether we want to model dates as Literal dates. How about modelling "temporal 
coverage" with an own uri, maybe giving it ical:dtstart, ical:dtend as descriptive properties, but 
most importantly giving it skos:notation with "Period from dtstart to dtend" or "Instant at dtstart"?
Also, one could then more or less automatically link to the date reference ontology for hierarchies,
e.g., within year 2011 or before 2012. Also, we could give it names such as "Q3, Q4" etc.
** ...

===How about querying over several cubes together?===
* Cube created
* Test query created
* Next steps
** Have to query over both datasets together
** Enable reasoning
** Try in Saiku


==2013-02-22==
* We have again refactored the use of square brackets: For "Measures", we keep the name without 
square brackets since Saiku is using those names specifically. For all others, the square brackets
are a necessary part of the name (e.g., since MDX parser would not work for a member that starts 
with a numeric character.

==2013-02-17 - Problem: .Member is interpreted as an Identifier segment==
* uri.Member is interpreted as a full Identifier of several segments
* Solutions: 
** How about defining a visitor that goes through the parser tree and transforms 
IdentifierNode(uri.Members) to CallNode(Members, Identifier(uri)).

==2013-02-18 - Problem: Literal values apparently require square brackets==
* MDX Parser does not work with 2009ZZZ08ZZZ31, only with [2009ZZZ08ZZZ31].
* Also, Saiku does not attach [] to the names when transformed to MDX.
* Therefore, the unique name should actually say [Measures].
* Again, we need to rethink the brackets approach:
** Since the unique name is used for building MDX queries, and since MDX requires it, we add it.
** When coming from MDX, we need to use toString(), since it keeps the square brackets.
** Also, we need to add [] if comparing.

==2013-02-17 - Problem: If asking for members, we do not consider Literal values==
* Of all multidimensional elements, members possibly can be described by Literal values.
* Thus, when querying for those members, we have to consider that.
* In theory this can happen in two cases:
** A codeList is described that defines concepts with skos:notation (not overly discussed, yet)
** No codeList is given, and the members are simply read from the observations (in the future, 
this case might be abandoned, since for cubes, we should always have a complete code list. However, 
in data cubes, often you have "degenerated" dimensions, thus we might consider keeping it)
* Problem: How do we know whether it is a URI member or Literal member? For now, we try just
guessing from the name. However, with prefixeduris this will not work.

* TODO: SPARQL query still does not work:
** Problem has been the fact that str(?MEMBER_UNIQUE_NAME) was needed.

==2013-02-16 - Still problem with squared brackets==
* Solution: Now that we fully encode a name (even dots), we do not need
squared brackets any more.
* We now try whether we can fully avoid them for the names (since with
LD, their unique identifier should be unique.
* Therefore I also do not need removeSquaredBrackets any more.

==2013-02-16 - Problem: Too many cubes==
* Get Cubes, maybe reasoning?

<pre>
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME "CUBE" as ?CUBE_TYPE ?CUBE_NAME as ?DESCRIPTION ?CUBE_NAME as ?CUBE_CAPTION  from <http://fios:2> where { ?ds qb:structure ?CUBE_NAME. ?CUBE_NAME a qb:DataStructureDefinition. OPTIONAL {?CUBE_NAME rdfs:label ?CUBE_CAPTION FILTER ( lang(?CUBE_CAPTION) = "" )} OPTIONAL {?CUBE_NAME rdfs:comment ?DESCRIPTION FILTER ( lang(?DESCRIPTION) = "" )} }order by ?CUBE_NAME limit 10
</pre>

==2013-02-14 - MDX-to-SPARQL debugging==
java.lang.RuntimeException: lookup on http://public.b-kaempgen.de:8890/sparql?query=PREFIX+dc%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E+%0APREFIX+sdmx-measure%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fsdmx%2F2009%2Fmeasure%23%3E+%0APREFIX+edgar%3A+%3Chttp%3A%2F%2Fedgarwrap.ontologycentral.com%2Fvocab%2F%3E+%0APREFIX+gesis-dbpedia-stats2%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2F%3E+%0APREFIX+smartdbwrap%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2F%3E+%0APREFIX+qb%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fcube%23%3E+%0APREFIX+refgovukyear%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fyear%2F%3E+%0APREFIX+refgovukmonth%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fmonth%2F%3E+%0APREFIX+dcterms%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Fterms%2F%3E+%0APREFIX+rdfs%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2000%2F01%2Frdf-schema%23%3E+%0APREFIX+skosclass%3A+%3Chttp%3A%2F%2Fddialliance.org%2Fontologies%2Fskosclass%23%3E+%0APREFIX+dropedia%3A+%3Chttp%3A%2F%2Fagkwebserver2.agk.uni-karlsruhe.de%2F%7Edropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+eus%3A+%3Chttp%3A%2F%2Fontologycentral.com%2F2009%2F01%2Feurostat%2Fns%23%3E+%0APREFIX+smartlocation%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Flocation%2F%3E+%0APREFIX+dbpedia%3A+%3Chttp%3A%2F%2Fdbpedia.org%2Fresource%2F%3E+%0APREFIX+owl%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2002%2F07%2Fowl%23%3E+%0APREFIX+smartanalysisobject%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Fanalysisobject%2F%3E+%0APREFIX+rdf%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%3E+%0APREFIX+rdfh%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23%3E+%0APREFIX+dropedialocal%3A+%3Chttp%3A%2F%2Flocalhost%2FDropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E+%0APREFIX+gesis-dbpedia-stats%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2Fns%23%3E+%0APREFIX+refgovukday%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fday%2F%3E+%0APREFIX+rdfh-inst%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23%3E+%0Aselect++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_ds%3E++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_dsd%3E+from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_levels%3E+where+%7B++%3Fobs+qb%3AdataSet+%3Fds.+%3Fds+qb%3Astructure+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23dsd%3E.%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey+FILTER%28%3Frdfhlocustkey+%3D+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23customer_178%3E+%29.%3Fobs+%3CMeasures%3E+%3FMeasures.+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey.+OPTIONAL+%7B+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_revenue%3E+%3Frdfhlorevenue.+%7D%7D+group+by++%3FMeasures++%3Frdfhlocustkey+order+by++%3FMeasures++%3Frdfhlocustkey+ resulted HTTP in status code 400
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparqlOpenVirtuoso(OpenVirtuosoEngine.java:487)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparql(OpenVirtuosoEngine.java:359)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.getOlapResult(OpenVirtuosoEngine.java:2232)
	at org.olap4j.driver.ld.LdOlap4jCellSet.cacheDataFromOlapQuery(LdOlap4jCellSet.java:342)
	at org.olap4j.driver.ld.LdOlap4jCellSet.populateFromMdx(LdOlap4jCellSet.java:139)
	at org.olap4j.driver.ld.LdOlap4jStatement.executeOlapQuery(LdOlap4jStatement.java:446)
	at org.olap4j.SSBQueryTest.executeStatement(SSBQueryTest.java:184)
	at org.olap4j.SSBQueryTest.testGenericQuery(SSBQueryTest.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

------------------------------------
Query:
Virtuoso 37000 Error SP030: SPARQL compiler, line 25: syntax error at 'from' before '<http://localhost:8890/DAV/ssb_01_qb_ds>'

SPARQL query:
define sql:big-data-const 0 PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select  from <http://localhost:8890/DAV/ssb_01_qb_ds>  from <http://localhost:8890/DAV/ssb_01_qb_dsd> from <http://localhost:8890/DAV/ssb_01_qb_levels> where {  ?obs qb:dataSet ?ds. ?ds qb:structure <http://lod2.eu/schemas/rdfh-inst#dsd>.?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey FILTER(?rdfhlocustkey = <http://lod2.eu/schemas/rdfh-inst#customer_178> ).?obs <Measures> ?Measures. ?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey. OPTIONAL { ?obs <http://lod2.eu/schemas/rdfh#lo_revenue> ?rdfhlorevenue. }} group by  ?Measures  ?rdfhlocustkey order by  ?Measures  ?rdfhlocustkey 

==2013-02-13 - From MDX to populated cellset==
1) MDX: 
SELECT 
<COLUMNAXIS> ON COLUMNS,
<ROWAXIS> ON ROWS,
FROM <CUBE>
WHERE <FILTERAXIS>

2) MdxMethodVisitor
<CUBE> => cube
<COLUMNAXIS> => list of positions for columns
<ROWAXIS> => list of positions for rows
<FILTERAXIS> => list of positions for filters
list of positions for columns => levels for column axis
list of positions for rows => levels for row axis
list of positions for filter => levels for filter axis

3) CellSetMetaData
cube => metadata
levels for column axis => hierarchies for column axis
levels for row axis => hierarchies for row axis
levels for filter axis => hierarchies for filter axis

4) OLAP query
cube => cube
levels for column axis + levels for row axis => slicesrollups
list of positions for filter => dices
check list of positions for mentioned members => projections

Pseudocode:

Algorithm 2: OLAP Query Generation
Input: metadata, axisList, filterCellSetAxis
Output: LdOlapQuery (cube, slicesrollups, dices, projections)

begin
  // Cube is easy
  cube = metadata.cube
  // Slicesrollups is easy
  slicesrollups = new List<Level>()
  for cellsetaxis \in axisList {
    position = cellsetaxis.positions(0)
    members = position.getMembers()
    for member \in members {
      // Only if no measure, since we collect measures later
      if (member.Type != Measure) {
        slicesrollups.add(member.getLevel())
      }
    }
  }
  // Problem Dices: From MDX, filterCellSetAxis returns a list of positions that should be ORed. Thus,
  // dices actually could be filled with positions.
  dices = filterCellSetAxis.positions
  // Problem Projections: We use a set, since we only want to have each Measure kept in there once.
  projections = new Set<Member>()
  for cellsetaxis \in axisList {
    for position \in cellsetaxis.positions {
      members = position.getMembers()
      for member \in members {
        if (member.Type == Measure) {
          projections.add(member)
        }
      }
    }
  }
  for position \in filterCellSetAxis.positions {
    members = position.getMembers()
    for member \in members {
      if (member.Type == Measure) {
        projections.add(member)
      }
    }
  }
  return new LdOlapQuery(cube, slicesrollups, dices, projections)

5) SPARQL query

Algorithm 1: OLAP Query Processing
Input: An OLAP Query (cube, SlicesRollups, Dices, Projections)
Output: A SPARQL query string

begin
  whereClause = graph pattern for observation in dataset described by data structure definition of cube
  for level \in SlicesRollups AND level != ALL do
    selectClause = selectClause + select for level
    levelHeight = level.getHeight()
    dimension = level.getHierarchy().getDimension()
    hashMap.put(dimension, levelHeight)
    for 0 to levelHeight do
      levelPath = levelPath + graph pattern for path via skos:narrower 
    levelPath = levelPath + graph pattern for level member via skos:member
    whereClause = whereClause + levelPath
    groupByClause = groupByClause + group by level
  // We assume that each position has the same metadata (i.e., Levels)
  position = Dices.positions.get(0)
  for member \in position {
    dicesLevelHeight = member.getLevel().getHeight()
    slicesRollupLevelHeight = hashMap.get(member.getLevel().getHierarchy().getDimension()
    levelPath = ""
    for 0 to slicesRollupLevelHeight - dicesLevelHeight {
      levelPath = levelPath + graph pattern for path via skos:narrower
    }
  } 
  for position \in Dices {
    for member \in Members do
      memberFilterAnd = memberFilterAnd + "AND" + filter for member
    memberFilter = memberFilter + "OR" + memberFilterAnd 
  }
  whereClause = whereClause + levelPath + memberFilter
  for measure \in Projections do
    aggregationSelect = measure.getAggregationFunction()
    selectClause = selectClause + aggregationSelect
    optionalPattern = optional graph pattern with measure
    whereClause = whereClause + optionalPattern
  query = selectClause + whereClause + groupByClause
  return query




==2013-02-07==
* Added to Tomcat for executing Saiku: "set CATALINA_OPTS=-Xms512m -Xmx768m -XX:MaxPermSize=256m -Dfile.encoding=UTF-8 
-Dorg.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true" , as explained here: 
http://ask.analytical-labs.com/questions/943/saiku-dont-execute-in-the-browser?focusedAnswerId=1143&sort=votes&page=2

* Logging refactored: Logging is enabled/disabled in LdOlap4jDriver.java (line 193):
** Disabled: LdOlap4jUtil._log.setLevel(Level.SEVERE);
** Enabled: LdOlap4jUtil._log.setLevel(Level.INFO);

===Problem: LinkedDataEngine may be called many times synchronously===
* So far, we have one LinkedDataEngine per Connection. 
* However, several metadata lists may get populated at the same time. Also, Saiku may be used by
several people at the same time. 
* Currently, the LinkedDataEngine has a state, since it has the current context stored as object
properties. 
* Solution: We create an internal class similar to "context", that stores the needed names from 
restrictions.  

===Problem: Created MDX in Saiku does not surround entity names with square brackets===
* The MDX parser requires entity names to be wrapped by square brackets.
* So far, only a cubes unique name is wrapped by square brackets. 
* All other elements are defined by XmlaOlap4jElement and they do not add square brackets.
* To consider
** Unique names of elements are used for filtering in SPARQL queries.
* Possible Solution: We just add the brackets as "MDX"-specific. 
** However, we should only transform SPARQL outputs if they are needed in MDX.
* Next problem: From segment identifier objects, the [] are already removed.
* 
* Any possibility to know the cube when we are looking for an identifier?

===Problem: How about tree operators===
* Currently, if tree operators are used, we assume that we are looking for a measure member.
* Now that we do not have applyRestrictions any more, it becomes more difficult.
* The following treeOps are possible:
** Interesting: 
	 * <p>The <code>treeOps</code> parameter allows you to retrieve members
     * relative to a given member. It is only applicable if a
     * <code>memberUniqueName</code> is also specified; otherwise it is
     * ignored. The following example retrieves all descendants and ancestors
     * of California, but not California itself:
** Parent, Siblings, Children, Self, Ancestors, Descendants

==2013-02-03==
* Problem: If we run a test on data sources with metadata queries, olap4ld does not yet know
how to translate prefixes to URIs. If I run OLAP queries, it does not since it populates the object
metadata and gives names to all multidimensional elements. However, for metadata queries, it does
not do it. After a metadata query is issued, for output, the elements are translated to MDX 
expressions, i.e., after output, the prefixes are known. That means, for metadata queries, we have 
to first query for cubes, then dimensions, then hierarchies etc. 

* Solution: We use prefixes as mentioned in the standardprefixes.csv. If we do not find prefixes,
we encode the URLs ourselves. This will not be pretty, but for those reasons we need labels.  
* Next solution: We now use Base64, since it would completely encode a URI and would hopefully 
not leave any problematic characters (http://blog.axxg.de/2012/02/java-kodierung-base64/)
* For that, we had to add apache commons (http://commons.apache.org/codec/apidocs/overview-summary.html)
* Note: Base64 had to be used with 0 linelength so that it would not add a linebreak.
* Encoding/Decoding yourself at: http://www.yellowpipe.com/yis/tools/encrypter/index.php

==2013-01-31==
* Measures and aggregation functions: We say that the aggregation function of a measure is inherent 
to the measure, i.e., it has to define itself how it aggregates if viewed from a lower granularity.
From a measure, you can always derive new measures.   

==2013-01-19==
* When filtering for members in SSB, I had to use function str(), otherwise I got an error "Virtuoso 22005 Error SR130 Bad type VARCHAR".

==2013-01-17==
* getSchemas and getCatalogs return different columns than previously used. See API.
* Wildcards disabled: http://www.w3schools.com/sql/sql_wildcards.asp

==2012-03-28==
* Encoding all to UTF-8
* Testing now works
* Implementing of SPARQL concept 

==2012-01-31==
* License added
* CROSSJOIN added

==2012-01-29==
* Add ordering for getXXX methods in LinkedDataEngine, according to [1]
* Add handling of names
* TODO: Clean up (old methods, VisitorClasses...) 


[1] <http://olap4j.svn.sourceforge.net/viewvc/olap4j/trunk/doc/olap4j_fs.html#The_OlapDatabaseMetaData_interface_and_methods_which_return_schema_rowsets>

==2012-01-28==
* Tested with Saiku-2.2RC1
* Filter axis supported
* Single measures supported

==2012-01-22==
* OLAP4LD does not require mondrian or MySQL server, anymore.
* No results are given, yet.

==2011-12-12==
* README added

==2011-11-21==
* Supports OpenVirtuoso triple store
