==2013-07-05==
* Problem: GetCubes() auf dem Schema gibt uns keinerlei Cubes, da das System nicht weiß, woher
er sie nehmen soll. Daher setzen wir auf getMeasures(Cube), getDimensions(Cube) etc., müssen hier
jedoch sicherstellen, dass zumindest der Datensatz aufgelöst worden ist.
** Lösung: We check the restrictions and at least check whether dataset is resolved (in which case,
the datastructuredefinition should have been resolved as well).
* with setCatalog() in Connection, I reset the LinkedDataEngine from now on. <= does not work!
** We reset at getSchemas() of Catalog. Also, we now load only if instance qb:DataSet.
* We inserted checks from specification for when qb:DataSet is loaded.

==2013-07-4==
* Problem came up that the following elements have the same name in our current model:
** Dimension, Dimension without codelist Hierarchy, Dimension without codelist Level
** Hierarchy, Regular hierarchy level
** Since we do not need to refer to those sub-elements, directly, but rather will be asking for all 
members of the dimension, we do not need to consider this.
* Measure DataType is now interpreted from rdfs:range.
* Watch out for the right name in MDX query.
* Sesame SPARQL adaption:
** Picky with empty spaces, added serveral
** (COUNT(?variable) as ?variable)
* xmla-server + xmla4js was quite picky about some non-implemented methods in olap4ld taken over 
from olap4j/mondrian (e.g., Olap4ldDatabase.getUserName()). Fixed those with workarounds. 
* We name Catalog and Schema the same due to compatibility reasons with xmla-server/xmla4js
* We have added measureList to CellSet as added metadata.

==2013-07-03==
* xml2nx adapted to Sesame SPARQL/XML output with linebreaks
* added manually created representative SSB example
* added Sesame SPARQL templates

==2013-07-02==
* Read in first location.
* Problem: A data cube is an instance of qb:DataSet. What do we
typically store with the qb:DataSet? For sure the outgoing link to dsd,
the outgoing link to all of its slices that in turn link to the
observations. Sometimes, we may have the observations already in the ds
location.
* Ideally, we get the following from...
** DS: Links from observations / Links to slices that in turn link to observations, rdfs:label, 
rdfs:comment, qb:structure.
** DSD: All information about the DSD, including code lists.

* For now, we assume (and adapt the SSB dataset accordingly):
** DS: Links from observations, rdfs:label, qb:structure
** DSD: All measure properties, dimension properties, code lists...

* Note in spec: If a dimension property has a qb:codeList, then the value of the dimension property on every qb:Observation must be in the code list. 
* Note in spec: Every qb:Observation has exactly one associated qb:DataSet. 
* Note in spec: Every qb:DataSet has exactly one associated qb:DataStructureDefinition. 
* Note in spec: "Every dimension declared in a qb:DataStructureDefinition must have a declared rdfs:range."
* Note in spec: "Every dimension with range skos:Concept must have a qb:codeList." <= This means, 
we do not necessarily need a code list in many cases. But, if we have a code list, then:
* "If a dimension property has a qb:codeList, then the value of the dimension property on every 
qb:Observation must be in the code list."

==2013-07-01 (before)==

* Warum haben die Dimensionen alle zwei Englische rdfs:label? 

===Problem: Saiku calls <Measures>===
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?HIERARCHY_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_CAPTION ?DIMENSION_UNIQUE_NAME as ?LEVEL_NAME "N/A" as ?DESCRIPTION "1" as ?LEVEL_NUMBER "0" as ?LEVEL_CARDINALITY "0x0000" as ?LEVEL_TYPE  from <http://fios:saiku>  where { ?CUBE_NAME qb:component ?compSpec . ?compSpec qb:dimension ?DIMENSION_UNIQUE_NAME. FILTER NOT EXISTS { ?DIMENSION_UNIQUE_NAME qb:codeList ?HIERARCHY_UNIQUE_NAME. }   FILTER (?CUBE_NAME = <http://public.b-kaempgen.de:8080/archive/MA/2007-06-06#dsd>)  FILTER (?DIMENSION_UNIQUE_NAME = <Measures>) } order by ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?LEVEL_NUMBER 


===Open issues===

* Reasoning does not work
** If we enable reasoning, the queries take much too long
** Also, we do not know what URIs are given to the members.
** Possible solution: We need to further normalise the data to use one specific uri per resource.

* Queries are very slow. Possible solutions:
** We could define code lists (hierarchies etc.)
** We could enable caching
** We could only gradually populate the multidimensional data

** How about predefining codelists?

* CAPTIONS
** We only use english captions
** We use skos:notation for members
** We use rdfs:label for everything else

* How about querying over several cubes together?
** We have to define the measures clearly. For SEC / Yahoo what is the appropriate aggregation function?
** Solution: AVG seems a good candidate for now. 
** Another solution: We separate the measures.

* How about the Member function?

* How about the time information?
** One part of the problem: In the current implementation, we cater for the special situation where
the observations use Literal values but we still would like to define code lists. In those cases,
we so far allowed to use skos:Concept and to give them skos:notations that could be used in the
observations. In FIOS 2.0, this would be the case for dates and for segment. However, we already use
skos:notation as a (unique) caption for a member. Therefore, we change this assumption and take
Literal observations automatically as "degenerated dimensions".
** Also, I am not sure whether we want to model dates as Literal dates. How about modelling "temporal 
coverage" with an own uri, maybe giving it ical:dtstart, ical:dtend as descriptive properties, but 
most importantly giving it skos:notation with "Period from dtstart to dtend" or "Instant at dtstart"?
Also, one could then more or less automatically link to the date reference ontology for hierarchies,
e.g., within year 2011 or before 2012. Also, we could give it names such as "Q3, Q4" etc.
** ...

===How about querying over several cubes together?===
* Cube created
* Test query created
* Next steps
** Have to query over both datasets together
** Enable reasoning
** Try in Saiku


==2013-02-22==
* We have again refactored the use of square brackets: For "Measures", we keep the name without 
square brackets since Saiku is using those names specifically. For all others, the square brackets
are a necessary part of the name (e.g., since MDX parser would not work for a member that starts 
with a numeric character.

==2013-02-17 - Problem: .Member is interpreted as an Identifier segment==
* uri.Member is interpreted as a full Identifier of several segments
* Solutions: 
** How about defining a visitor that goes through the parser tree and transforms 
IdentifierNode(uri.Members) to CallNode(Members, Identifier(uri)).

==2013-02-18 - Problem: Literal values apparently require square brackets==
* MDX Parser does not work with 2009ZZZ08ZZZ31, only with [2009ZZZ08ZZZ31].
* Also, Saiku does not attach [] to the names when transformed to MDX.
* Therefore, the unique name should actually say [Measures].
* Again, we need to rethink the brackets approach:
** Since the unique name is used for building MDX queries, and since MDX requires it, we add it.
** When coming from MDX, we need to use toString(), since it keeps the square brackets.
** Also, we need to add [] if comparing.

==2013-02-17 - Problem: If asking for members, we do not consider Literal values==
* Of all multidimensional elements, members possibly can be described by Literal values.
* Thus, when querying for those members, we have to consider that.
* In theory this can happen in two cases:
** A codeList is described that defines concepts with skos:notation (not overly discussed, yet)
** No codeList is given, and the members are simply read from the observations (in the future, 
this case might be abandoned, since for cubes, we should always have a complete code list. However, 
in data cubes, often you have "degenerated" dimensions, thus we might consider keeping it)
* Problem: How do we know whether it is a URI member or Literal member? For now, we try just
guessing from the name. However, with prefixeduris this will not work.

* TODO: SPARQL query still does not work:
** Problem has been the fact that str(?MEMBER_UNIQUE_NAME) was needed.

==2013-02-16 - Still problem with squared brackets==
* Solution: Now that we fully encode a name (even dots), we do not need
squared brackets any more.
* We now try whether we can fully avoid them for the names (since with
LD, their unique identifier should be unique.
* Therefore I also do not need removeSquaredBrackets any more.

==2013-02-16 - Problem: Too many cubes==
* Get Cubes, maybe reasoning?

<pre>
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME "CUBE" as ?CUBE_TYPE ?CUBE_NAME as ?DESCRIPTION ?CUBE_NAME as ?CUBE_CAPTION  from <http://fios:2> where { ?ds qb:structure ?CUBE_NAME. ?CUBE_NAME a qb:DataStructureDefinition. OPTIONAL {?CUBE_NAME rdfs:label ?CUBE_CAPTION FILTER ( lang(?CUBE_CAPTION) = "" )} OPTIONAL {?CUBE_NAME rdfs:comment ?DESCRIPTION FILTER ( lang(?DESCRIPTION) = "" )} }order by ?CUBE_NAME limit 10
</pre>

==2013-02-14 - MDX-to-SPARQL debugging==
java.lang.RuntimeException: lookup on http://public.b-kaempgen.de:8890/sparql?query=PREFIX+dc%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E+%0APREFIX+sdmx-measure%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fsdmx%2F2009%2Fmeasure%23%3E+%0APREFIX+edgar%3A+%3Chttp%3A%2F%2Fedgarwrap.ontologycentral.com%2Fvocab%2F%3E+%0APREFIX+gesis-dbpedia-stats2%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2F%3E+%0APREFIX+smartdbwrap%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2F%3E+%0APREFIX+qb%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fcube%23%3E+%0APREFIX+refgovukyear%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fyear%2F%3E+%0APREFIX+refgovukmonth%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fmonth%2F%3E+%0APREFIX+dcterms%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Fterms%2F%3E+%0APREFIX+rdfs%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2000%2F01%2Frdf-schema%23%3E+%0APREFIX+skosclass%3A+%3Chttp%3A%2F%2Fddialliance.org%2Fontologies%2Fskosclass%23%3E+%0APREFIX+dropedia%3A+%3Chttp%3A%2F%2Fagkwebserver2.agk.uni-karlsruhe.de%2F%7Edropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+eus%3A+%3Chttp%3A%2F%2Fontologycentral.com%2F2009%2F01%2Feurostat%2Fns%23%3E+%0APREFIX+smartlocation%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Flocation%2F%3E+%0APREFIX+dbpedia%3A+%3Chttp%3A%2F%2Fdbpedia.org%2Fresource%2F%3E+%0APREFIX+owl%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2002%2F07%2Fowl%23%3E+%0APREFIX+smartanalysisobject%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Fanalysisobject%2F%3E+%0APREFIX+rdf%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%3E+%0APREFIX+rdfh%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23%3E+%0APREFIX+dropedialocal%3A+%3Chttp%3A%2F%2Flocalhost%2FDropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E+%0APREFIX+gesis-dbpedia-stats%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2Fns%23%3E+%0APREFIX+refgovukday%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fday%2F%3E+%0APREFIX+rdfh-inst%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23%3E+%0Aselect++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_ds%3E++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_dsd%3E+from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_levels%3E+where+%7B++%3Fobs+qb%3AdataSet+%3Fds.+%3Fds+qb%3Astructure+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23dsd%3E.%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey+FILTER%28%3Frdfhlocustkey+%3D+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23customer_178%3E+%29.%3Fobs+%3CMeasures%3E+%3FMeasures.+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey.+OPTIONAL+%7B+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_revenue%3E+%3Frdfhlorevenue.+%7D%7D+group+by++%3FMeasures++%3Frdfhlocustkey+order+by++%3FMeasures++%3Frdfhlocustkey+ resulted HTTP in status code 400
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparqlOpenVirtuoso(OpenVirtuosoEngine.java:487)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparql(OpenVirtuosoEngine.java:359)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.getOlapResult(OpenVirtuosoEngine.java:2232)
	at org.olap4j.driver.ld.LdOlap4jCellSet.cacheDataFromOlapQuery(LdOlap4jCellSet.java:342)
	at org.olap4j.driver.ld.LdOlap4jCellSet.populateFromMdx(LdOlap4jCellSet.java:139)
	at org.olap4j.driver.ld.LdOlap4jStatement.executeOlapQuery(LdOlap4jStatement.java:446)
	at org.olap4j.SSBQueryTest.executeStatement(SSBQueryTest.java:184)
	at org.olap4j.SSBQueryTest.testGenericQuery(SSBQueryTest.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

------------------------------------
Query:
Virtuoso 37000 Error SP030: SPARQL compiler, line 25: syntax error at 'from' before '<http://localhost:8890/DAV/ssb_01_qb_ds>'

SPARQL query:
define sql:big-data-const 0 PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select  from <http://localhost:8890/DAV/ssb_01_qb_ds>  from <http://localhost:8890/DAV/ssb_01_qb_dsd> from <http://localhost:8890/DAV/ssb_01_qb_levels> where {  ?obs qb:dataSet ?ds. ?ds qb:structure <http://lod2.eu/schemas/rdfh-inst#dsd>.?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey FILTER(?rdfhlocustkey = <http://lod2.eu/schemas/rdfh-inst#customer_178> ).?obs <Measures> ?Measures. ?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey. OPTIONAL { ?obs <http://lod2.eu/schemas/rdfh#lo_revenue> ?rdfhlorevenue. }} group by  ?Measures  ?rdfhlocustkey order by  ?Measures  ?rdfhlocustkey 

==2013-02-13 - From MDX to populated cellset==
1) MDX: 
SELECT 
<COLUMNAXIS> ON COLUMNS,
<ROWAXIS> ON ROWS,
FROM <CUBE>
WHERE <FILTERAXIS>

2) MdxMethodVisitor
<CUBE> => cube
<COLUMNAXIS> => list of positions for columns
<ROWAXIS> => list of positions for rows
<FILTERAXIS> => list of positions for filters
list of positions for columns => levels for column axis
list of positions for rows => levels for row axis
list of positions for filter => levels for filter axis

3) CellSetMetaData
cube => metadata
levels for column axis => hierarchies for column axis
levels for row axis => hierarchies for row axis
levels for filter axis => hierarchies for filter axis

4) OLAP query
cube => cube
levels for column axis + levels for row axis => slicesrollups
list of positions for filter => dices
check list of positions for mentioned members => projections

Pseudocode:

Algorithm 2: OLAP Query Generation
Input: metadata, axisList, filterCellSetAxis
Output: LdOlapQuery (cube, slicesrollups, dices, projections)

begin
  // Cube is easy
  cube = metadata.cube
  // Slicesrollups is easy
  slicesrollups = new List<Level>()
  for cellsetaxis \in axisList {
    position = cellsetaxis.positions(0)
    members = position.getMembers()
    for member \in members {
      // Only if no measure, since we collect measures later
      if (member.Type != Measure) {
        slicesrollups.add(member.getLevel())
      }
    }
  }
  // Problem Dices: From MDX, filterCellSetAxis returns a list of positions that should be ORed. Thus,
  // dices actually could be filled with positions.
  dices = filterCellSetAxis.positions
  // Problem Projections: We use a set, since we only want to have each Measure kept in there once.
  projections = new Set<Member>()
  for cellsetaxis \in axisList {
    for position \in cellsetaxis.positions {
      members = position.getMembers()
      for member \in members {
        if (member.Type == Measure) {
          projections.add(member)
        }
      }
    }
  }
  for position \in filterCellSetAxis.positions {
    members = position.getMembers()
    for member \in members {
      if (member.Type == Measure) {
        projections.add(member)
      }
    }
  }
  return new LdOlapQuery(cube, slicesrollups, dices, projections)

5) SPARQL query

Algorithm 1: OLAP Query Processing
Input: An OLAP Query (cube, SlicesRollups, Dices, Projections)
Output: A SPARQL query string

begin
  whereClause = graph pattern for observation in dataset described by data structure definition of cube
  for level \in SlicesRollups AND level != ALL do
    selectClause = selectClause + select for level
    levelHeight = level.getHeight()
    dimension = level.getHierarchy().getDimension()
    hashMap.put(dimension, levelHeight)
    for 0 to levelHeight do
      levelPath = levelPath + graph pattern for path via skos:narrower 
    levelPath = levelPath + graph pattern for level member via skos:member
    whereClause = whereClause + levelPath
    groupByClause = groupByClause + group by level
  // We assume that each position has the same metadata (i.e., Levels)
  position = Dices.positions.get(0)
  for member \in position {
    dicesLevelHeight = member.getLevel().getHeight()
    slicesRollupLevelHeight = hashMap.get(member.getLevel().getHierarchy().getDimension()
    levelPath = ""
    for 0 to slicesRollupLevelHeight - dicesLevelHeight {
      levelPath = levelPath + graph pattern for path via skos:narrower
    }
  } 
  for position \in Dices {
    for member \in Members do
      memberFilterAnd = memberFilterAnd + "AND" + filter for member
    memberFilter = memberFilter + "OR" + memberFilterAnd 
  }
  whereClause = whereClause + levelPath + memberFilter
  for measure \in Projections do
    aggregationSelect = measure.getAggregationFunction()
    selectClause = selectClause + aggregationSelect
    optionalPattern = optional graph pattern with measure
    whereClause = whereClause + optionalPattern
  query = selectClause + whereClause + groupByClause
  return query




==2013-02-07==
* Added to Tomcat for executing Saiku: "set CATALINA_OPTS=-Xms512m -Xmx768m -XX:MaxPermSize=256m -Dfile.encoding=UTF-8 
-Dorg.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true" , as explained here: 
http://ask.analytical-labs.com/questions/943/saiku-dont-execute-in-the-browser?focusedAnswerId=1143&sort=votes&page=2

* Logging refactored: Logging is enabled/disabled in LdOlap4jDriver.java (line 193):
** Disabled: LdOlap4jUtil._log.setLevel(Level.SEVERE);
** Enabled: LdOlap4jUtil._log.setLevel(Level.INFO);

===Problem: LinkedDataEngine may be called many times synchronously===
* So far, we have one LinkedDataEngine per Connection. 
* However, several metadata lists may get populated at the same time. Also, Saiku may be used by
several people at the same time. 
* Currently, the LinkedDataEngine has a state, since it has the current context stored as object
properties. 
* Solution: We create an internal class similar to "context", that stores the needed names from 
restrictions.  

===Problem: Created MDX in Saiku does not surround entity names with square brackets===
* The MDX parser requires entity names to be wrapped by square brackets.
* So far, only a cubes unique name is wrapped by square brackets. 
* All other elements are defined by XmlaOlap4jElement and they do not add square brackets.
* To consider
** Unique names of elements are used for filtering in SPARQL queries.
* Possible Solution: We just add the brackets as "MDX"-specific. 
** However, we should only transform SPARQL outputs if they are needed in MDX.
* Next problem: From segment identifier objects, the [] are already removed.
* 
* Any possibility to know the cube when we are looking for an identifier?

===Problem: How about tree operators===
* Currently, if tree operators are used, we assume that we are looking for a measure member.
* Now that we do not have applyRestrictions any more, it becomes more difficult.
* The following treeOps are possible:
** Interesting: 
	 * <p>The <code>treeOps</code> parameter allows you to retrieve members
     * relative to a given member. It is only applicable if a
     * <code>memberUniqueName</code> is also specified; otherwise it is
     * ignored. The following example retrieves all descendants and ancestors
     * of California, but not California itself:
** Parent, Siblings, Children, Self, Ancestors, Descendants

==2013-02-03==
* Problem: If we run a test on data sources with metadata queries, olap4ld does not yet know
how to translate prefixes to URIs. If I run OLAP queries, it does not since it populates the object
metadata and gives names to all multidimensional elements. However, for metadata queries, it does
not do it. After a metadata query is issued, for output, the elements are translated to MDX 
expressions, i.e., after output, the prefixes are known. That means, for metadata queries, we have 
to first query for cubes, then dimensions, then hierarchies etc. 

* Solution: We use prefixes as mentioned in the standardprefixes.csv. If we do not find prefixes,
we encode the URLs ourselves. This will not be pretty, but for those reasons we need labels.  
* Next solution: We now use Base64, since it would completely encode a URI and would hopefully 
not leave any problematic characters (http://blog.axxg.de/2012/02/java-kodierung-base64/)
* For that, we had to add apache commons (http://commons.apache.org/codec/apidocs/overview-summary.html)
* Note: Base64 had to be used with 0 linelength so that it would not add a linebreak.
* Encoding/Decoding yourself at: http://www.yellowpipe.com/yis/tools/encrypter/index.php

==2013-01-31==
* Measures and aggregation functions: We say that the aggregation function of a measure is inherent 
to the measure, i.e., it has to define itself how it aggregates if viewed from a lower granularity.
From a measure, you can always derive new measures.   

==2013-01-19==
* When filtering for members in SSB, I had to use function str(), otherwise I got an error "Virtuoso 22005 Error SR130 Bad type VARCHAR".

==2013-01-17==
* getSchemas and getCatalogs return different columns than previously used. See API.
* Wildcards disabled: http://www.w3schools.com/sql/sql_wildcards.asp

==2012-03-28==
* Encoding all to UTF-8
* Testing now works
* Implementing of SPARQL concept 

==2012-01-31==
* License added
* CROSSJOIN added

==2012-01-29==
* Add ordering for getXXX methods in LinkedDataEngine, according to [1]
* Add handling of names
* TODO: Clean up (old methods, VisitorClasses...) 


[1] <http://olap4j.svn.sourceforge.net/viewvc/olap4j/trunk/doc/olap4j_fs.html#The_OlapDatabaseMetaData_interface_and_methods_which_return_schema_rowsets>

==2012-01-28==
* Tested with Saiku-2.2RC1
* Filter axis supported
* Single measures supported

==2012-01-22==
* OLAP4LD does not require mondrian or MySQL server, anymore.
* No results are given, yet.

==2011-12-12==
* README added

==2011-11-21==
* Supports OpenVirtuoso triple store
